{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66f42343-3976-431f-9c7a-f4a205043970",
   "metadata": {},
   "source": [
    "# Introduction of Statistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b7b119-17a0-4ad7-bfbf-a53a10931038",
   "metadata": {},
   "source": [
    "Probability Theory\n",
    "- Subjective and Objective Probabilities\n",
    "- Probability Spaces\n",
    "- Random Variables -> $X \\text{ is a measurable function}$\n",
    "- Expectation of a Random Variable -> $ E[f(X)] := \\int_{\\Omega_X} f(x) dP_X(x) $\n",
    "- Probability Density Functions -> $\\text{ a non negative function with integral} \\int \\text{ equals to 1}$\n",
    "- Joint Random Variable and Joint Distribution\n",
    "- Conditional Probabilities\n",
    "- Independence of Random Variables\n",
    "\n",
    "Estimation Theory\n",
    "- Mean Estimation Problem -> $ \\mu := E_{X \\sim P}[X] = \\int x \\cdot dP(x) \\, $\n",
    "- Data Generation Process -> $ \\, X_1, \\dots, X_N \\sim P (\\text{i.i.d}) \\, \\text{independently and identically distributed}$\n",
    "- Preliminaries: Expectation and Variance\n",
    "   > Some Key Properties of Expectation:\n",
    "   > \n",
    "   > $E[cX] = cE[X]$, $E[X + Y ] = E[X] + E[Y ]$, $E[XY ] = E[X]E[Y ]$\n",
    "   >\n",
    "   > Some Key Properties of Variance:\n",
    "   > \n",
    "   > $V[X] := E[(X − E[X])^2]$ = $\\int (x − E[X])^2 d P(x) ≥ 0$.\n",
    "   >\n",
    "   > $V[X] = E[X2] − (E[X])^2$\n",
    "   >\n",
    "   > $V[cX] = c^2V[X]$, $V[\\frac{X}{n}] = \\frac{1}{n^2}V[X]$.\n",
    "- Statistical Estimators\n",
    "- Mean Square Error and Bias-Variance Decomposition\n",
    "\n",
    "Squared Error -> $(\\hat{\\theta}_n − \\theta^∗)^2 = (Fn(Dn) − \\theta^∗)^2.$\n",
    "\n",
    "Bias-Variance Decomposition -> $E[(\\hat{\\theta}_n − θ^∗)^2] = \\frac{E[(\\hat{\\theta}_n − E[\\hat{\\theta}_n])^2]}{\\widehat{Variance}} + \\frac{(E[\\hat{\\theta}_n] − θ^∗)^2}{\\widehat{Bias}}$\n",
    "\n",
    "- Consistency and Unbiasedness\n",
    "- Variance Reduction by Introducing a Bias\n",
    "\n",
    "Maximum Likelihood Estimation\n",
    "- Estimation in Parametric Models\n",
    "- Maximum Likelihood Estimation -> $\\text{likelihood function } \\mathscr{ln} \\text{ is the product of PDFs} $\n",
    "- MLE as KL Divergence Minimization\n",
    "\n",
    "$ D_{\\text{KL}}(P || Q) = \\int_{-\\infty}^{\\infty} p(x) \\log \\frac{p(x)}{q(x)} \\, dx $\n",
    "\n",
    "- Consistency of MLE\n",
    "\n",
    "Hypothesis Testing\n",
    "- The Lady Tasting Tea Experiment\n",
    "- Procedure of Hypothesis Testing\n",
    "- Type I and Type II Errors\n",
    "- Test Statistics\n",
    "- P-Value\n",
    "- Neyman-Pearson Lemma and Likelihood Ratio Test $ \\text{LLR} = \\log \\frac{L(\\mathbf{x} | H_1)}{L(\\mathbf{x} | H_0)} $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4eb080-ea40-4e6b-8303-06b2eaa0327e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
