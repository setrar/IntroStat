{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cc8b01f-d071-4996-9140-73fe09d5a5e2",
   "metadata": {},
   "source": [
    "Maximum Likelihood Estimation (MLE) and Least Squares Estimation (LSE) are two fundamental approaches in statistical estimation and inference, each with its principles and applications. While both are used to estimate the parameters of a model given a set of observations, they differ in their methodologies, assumptions, and the contexts in which they are most effectively applied.\n",
    "\n",
    "### Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "1. **Principle**: MLE seeks the parameter values that maximize the likelihood function, which represents the probability of observing the given data under a specific statistical model. The likelihood is a function of the parameters of the model.\n",
    "\n",
    "2. **Assumptions**: It requires assumptions about the statistical distribution that the data follows. The choice of distribution (e.g., normal, binomial) is crucial for the formulation of the likelihood function.\n",
    "\n",
    "3. **Application**: MLE is widely used in various fields for parameter estimation when the underlying distribution of the data is known or assumed. It is especially powerful in complex models, including those where parameters are not linearly related to the predictors.\n",
    "\n",
    "4. **Properties**: Under certain regularity conditions, MLE estimators are asymptotically unbiased, consistent, and efficient (i.e., they achieve the lowest possible variance among all unbiased estimators).\n",
    "\n",
    "### Least Squares Estimation (LSE)\n",
    "\n",
    "1. **Principle**: LSE seeks the parameter values that minimize the sum of the squared differences (residuals) between the observed values and the values predicted by the model. It focuses on minimizing the error in terms of the Euclidean distance between observed and predicted values.\n",
    "\n",
    "2. **Assumptions**: LSE often assumes that the relationship between the model parameters and the response variable is linear. However, nonlinear least squares methods also exist. The basic form assumes homoscedasticity (constant variance of errors) and independence of errors.\n",
    "\n",
    "3. **Application**: LSE is the foundation of linear regression and is extensively used in fitting linear models. It is preferred for its simplicity and computational efficiency in problems where the relationship between variables is expected to be linear or approximated as linear.\n",
    "\n",
    "4. **Properties**: LSE estimators are unbiased in linear regression models with normally distributed errors. They are also consistent and efficient under the Gauss-Markov theorem, which does not require the normality assumption.\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "- **Foundation**: MLE is based on probability and focuses on maximizing the likelihood of observing the data given the parameters, while LSE is based on geometry/minimization of error and focuses on minimizing the discrepancy between observed values and those predicted by the model.\n",
    "- **Assumptions**: MLE requires assumptions about the entire probability distribution of the data, whereas LSE typically revolves around the assumption of a linear relationship and homoscedasticity.\n",
    "- **Application Contexts**: MLE is more versatile in handling complex models and distributions, making it suitable for a wide range of statistical models beyond linear regression. LSE is predominantly used in linear models and for problems where minimizing the error sum of squares is directly relevant.\n",
    "\n",
    "Both methods have their strengths and are chosen based on the specifics of the statistical problem at hand, including the underlying assumptions about the data and the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df85af93-11f2-485c-9dc2-c55f46d0eccd",
   "metadata": {},
   "source": [
    "To illustrate both Maximum Likelihood Estimation (MLE) and Least Squares Estimation (LSE) in Julia, let's consider simple examples: estimating the parameters of a normal distribution using MLE and fitting a linear model using LSE.\n",
    "\n",
    "### Example 1: Maximum Likelihood Estimation (MLE) for a Normal Distribution\n",
    "\n",
    "Suppose we have a sample from a normal distribution, and we want to estimate its mean ($\\mu$) and standard deviation ($\\sigma$) using MLE.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "447e613f-f4d0-40bc-9ed2-4714c29d2d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE Estimates:\n",
      "Mean (μ) estimate: 1.9666666666666666\n",
      "Standard Deviation (σ) estimate: 0.4546060565661952\n"
     ]
    }
   ],
   "source": [
    "using Statistics\n",
    "\n",
    "# Sample data\n",
    "data = [1.2, 2.3, 2.1, 1.8, 2.5, 1.9]\n",
    "\n",
    "# MLE estimations for normal distribution\n",
    "μ_hat = mean(data)\n",
    "σ_hat = std(data)\n",
    "\n",
    "println(\"MLE Estimates:\")\n",
    "println(\"Mean (μ) estimate: \", μ_hat)\n",
    "println(\"Standard Deviation (σ) estimate: \", σ_hat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4904ee90-1346-499f-bc2b-eb19068ac334",
   "metadata": {},
   "source": [
    "This code calculates the MLE estimates for the parameters of a normal distribution directly, as the MLE for a normal distribution's mean and standard deviation are the sample mean and standard deviation, respectively.\n",
    "\n",
    "### Example 2: Least Squares Estimation (LSE) for Linear Regression\n",
    "\n",
    "For LSE, we'll fit a simple linear model to data points. We'll use Julia's `GLM` package for linear regression, which inherently uses the least squares method for estimating the coefficients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7af88f7e-bd09-4d31-a76e-2f0c42fcdfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSE Linear Regression Coefficients:\n",
      "[2.1999999999999993, 0.6000000000000003]\n"
     ]
    }
   ],
   "source": [
    "using GLM\n",
    "using DataFrames\n",
    "\n",
    "# Sample data\n",
    "data = DataFrame(X = [1, 2, 3, 4, 5], Y = [2, 4, 5, 4, 5])\n",
    "\n",
    "# Fit a linear model\n",
    "model = lm(@formula(Y ~ X), data)\n",
    "\n",
    "println(\"LSE Linear Regression Coefficients:\")\n",
    "println(coef(model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07b082f-d621-4ebf-9c0b-a198fdf21f69",
   "metadata": {},
   "source": [
    "Before running this code, ensure you have the necessary packages installed:\n",
    "\n",
    "```julia\n",
    "using Pkg\n",
    "Pkg.add(\"GLM\")\n",
    "Pkg.add(\"DataFrames\")\n",
    "```\n",
    "\n",
    "This example demonstrates fitting a linear regression model to the `data` DataFrame using the `lm` function from the `GLM` package, which implements the LSE method. The model estimates the relationship between `X` and `Y` by minimizing the sum of squared residuals between the observed and predicted `Y` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4255bd0f-8139-47b9-9873-f732619d89c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
