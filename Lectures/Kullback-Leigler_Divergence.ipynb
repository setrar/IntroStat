{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1f6bbd0-35cc-4e99-aa39-c486cdd8521c",
   "metadata": {},
   "source": [
    "### What is the metric to compare distributions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dd4e20-0ad0-48ee-926c-21c73239732e",
   "metadata": {},
   "source": [
    "There are several metrics commonly used to compare distributions, depending on the specific characteristics of the data and the purpose of the comparison. Some of the most widely used metrics include:\n",
    "\n",
    "1. **Kolmogorov-Smirnov (KS) Test**: This test quantifies a distance between the empirical distribution functions of two samples. It's sensitive to differences in both location and shape of the empirical cumulative distribution functions.\n",
    "\n",
    "2. **Kullback-Leibler Divergence (KL Divergence)**: KL Divergence measures the difference between two probability distributions. It's not symmetric and is often used to compare a true probability distribution to an estimated one.\n",
    "\n",
    "3. **Jensen-Shannon Divergence (JS Divergence)**: JS Divergence is a symmetric and smoothed version of KL Divergence. It measures the similarity between two probability distributions.\n",
    "\n",
    "4. **Earth Mover's Distance (EMD)**: Also known as Wasserstein distance, it measures the distance between two probability distributions as the minimum amount of work required to transform one distribution into the other. It's particularly useful when comparing distributions with similar shapes but different scales.\n",
    "\n",
    "5. **Chi-Square Test**: This test measures how the observed distribution of categorical data differs from the expected distribution. It's commonly used in contingency table analysis.\n",
    "\n",
    "6. **Cramér's V**: Cramér's V is a measure of association between two nominal variables. It's based on chi-square statistics and varies between 0 (no association) and 1 (complete association).\n",
    "\n",
    "7. **Total Variation Distance (TVD)**: TVD measures the largest possible difference between the probability that the two distributions assign to any event. It's a metric of how far apart the two distributions are from each other.\n",
    "\n",
    "The choice of metric depends on the specific context and the nature of the distributions being compared. Some metrics are more suitable for continuous distributions, while others are designed for discrete or categorical data. It's important to consider the properties of the data and the goals of the comparison when selecting an appropriate metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fc98be-96e1-4174-b35f-04278512351d",
   "metadata": {},
   "source": [
    "### What is mutual information?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9b1eed-3cc6-4936-9e60-1b7b11f4a34a",
   "metadata": {},
   "source": [
    "Mutual information is a measure of the amount of information that one random variable contains about another random variable. In other words, it quantifies the degree of dependence between two variables.\n",
    "\n",
    "Mathematically, the mutual information between two random variables X and Y, denoted by I(X;Y), is defined as the reduction in uncertainty about one variable (say, X) when the other variable (Y) is known. It can be expressed using the entropy of the individual variables and the joint entropy of both variables:\n",
    "\n",
    "$$I(X;Y) = H(X) - H(X|Y)$$\n",
    "\n",
    "Where:\n",
    "- $H(X)$ is the entropy of variable X, which measures the uncertainty or randomness associated with X.\n",
    "- $H(X|Y)$ is the conditional entropy of X given Y, which measures the remaining uncertainty about X after Y is known.\n",
    "\n",
    "Alternatively, mutual information can be expressed as:\n",
    "\n",
    "$$I(X;Y) = H(X) + H(Y) - H(X,Y)$$\n",
    "\n",
    "Where:\n",
    "- $H(Y)$ is the entropy of variable Y.\n",
    "- $H(X,Y)$ is the joint entropy of variables X and Y, which measures the uncertainty associated with both variables simultaneously.\n",
    "\n",
    "Mutual information is non-negative (i.e., $I(X;Y) \\geq 0$) and symmetric, meaning $I(X;Y) = I(Y;X)$. It reaches its maximum value when X and Y are perfectly dependent and decreases as the variables become more independent.\n",
    "\n",
    "Mutual information is widely used in various fields, including information theory, statistics, machine learning, and signal processing. It's commonly used for feature selection, measuring the relationship between variables, and as a similarity measure in clustering algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71888ae-ac83-4f62-83fe-6a3f7693f7bb",
   "metadata": {},
   "source": [
    "### generate an example of Kullback-Leibler Divergence in Julia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "200488f2-3188-4190-9d7f-dc1075207057",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "864e4af3-0dc8-4a76-85fa-5b6bba3060a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Divergence between dist1 and dist2: -6.0:0.12121212121212122:6.0\n"
     ]
    }
   ],
   "source": [
    "# Define two probability distributions\n",
    "# For this example, let's use Normal distributions\n",
    "μ1, σ1 = 0.0, 1.0  # parameters for the first normal distribution\n",
    "μ2, σ2 = 0.0, 2.0  # parameters for the second normal distribution\n",
    "dist1 = Normal(μ1, σ1)\n",
    "dist2 = Normal(μ2, σ2)\n",
    "\n",
    "# Define a function to compute the KL divergence\n",
    "function kl_divergence(p::Normal{Float64}, q::Normal{Float64}; grid_length::Int64 = 100)\n",
    "    μ_p, σ_p = mean(p), std(p)\n",
    "    μ_q, σ_q = mean(q), std(q)\n",
    "    \n",
    "    # Determine reasonable finite bounds based on the distributions' parameters\n",
    "    start = min(μ_p, μ_q) - 3 * max(σ_p, σ_q)\n",
    "    stop = max(μ_p, μ_q) + 3 * max(σ_p, σ_q)\n",
    "    \n",
    "    # Use these bounds to generate the grid\n",
    "    x_grid = range(start, stop, length = grid_length)\n",
    "    \n",
    "    # Rest of the KL divergence calculation...\n",
    "end\n",
    "\n",
    "# Compute the KL divergence between dist1 and dist2\n",
    "kl_div = kl_divergence(dist1, dist2)\n",
    "println(\"KL Divergence between dist1 and dist2: \", kl_div)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5428d7a6-7219-4669-9efd-1568d1e063b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.3",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
